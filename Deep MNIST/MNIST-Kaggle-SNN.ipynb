{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST using Self Normalizing Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sethuiyer\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD8CAYAAADAKumpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1ILTrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfae10314e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def visualize_digits(i):\n",
    "    pixel_value_i = train_data.ix[i][1:]\n",
    "    pixel_value_i = pixel_value_i.values.reshape([28,28])\n",
    "    plt.imshow(pixel_value_i,cmap='Greys')\n",
    "    plt.text(28.,28.,'Label : ' + str(train_data.ix[i]['label']))\n",
    "    plt.show()\n",
    "visualize_digits(192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_numpy=np.array(train_data)\n",
    "X_train = train_data_numpy[:,1:]\n",
    "Y_train=train_data_numpy[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from selu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "training_epochs = 20\n",
    "batch_size = 512\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 784 # 1st layer number of features\n",
    "n_hidden_2 = 784 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input],name=\"train_inp\")\n",
    "y = tf.placeholder(\"float\", [None, n_classes],name=\"train_out\")\n",
    "dropoutRate = tf.placeholder(tf.float32,name=\"dropout\")\n",
    "is_training= tf.placeholder(tf.bool,name=\"is_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sethuiyer\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = './logs/'  #tensorboard\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases, rate, is_training):\n",
    "    # Hidden layer with SELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #netI_1 = layer_1\n",
    "    layer_1 = selu(layer_1)\n",
    "    layer_1 = dropout_selu(layer_1,rate, training=is_training)\n",
    "    \n",
    "    # Hidden layer with SELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    #netI_2 = layer_2\n",
    "    layer_2 = selu(layer_2)\n",
    "    layer_2 = dropout_selu(layer_2,rate, training=is_training)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=np.sqrt(1/n_input))),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=np.sqrt(1/n_hidden_1))),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=np.sqrt(1/n_hidden_2)))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1],stddev=0)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2],stddev=0)),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes],stddev=0))\n",
    "}\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases, rate=dropoutRate, is_training=is_training)\n",
    "correct_pred=tf.argmax(pred,1)\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    " # Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "         \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a histogramm for weights\n",
    "tf.summary.histogram(\"weights2\", weights['h2'])\n",
    "tf.summary.histogram(\"weights1\", weights['h1'])\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    i = 0\n",
    "    one_hot_y=np.zeros((batch_size,n_classes))\n",
    "    while True:\n",
    "        if i + batch_size >= len(train_data_numpy):\n",
    "            batch_x = X_train[i:]\n",
    "            batch_x=np.concatenate((batch_x,X_train[0:batch_size+i-len(train_data_numpy)]),axis=0)\n",
    "            batch_y = Y_train[i:]\n",
    "            batch_y=np.concatenate((batch_y,Y_train[0:batch_size+i-len(train_data_numpy)]),axis=0)\n",
    "            one_hot_y[np.arange(batch_size),batch_y] = 1  #convert to one hot encoding\n",
    "            batch_y = one_hot_y\n",
    "            one_hot_y=np.zeros((batch_size,n_classes))  #resetting it for future use\n",
    "            yield batch_x,batch_y\n",
    "        else:\n",
    "            batch_x = X_train[i:i+batch_size]\n",
    "            batch_y = Y_train[i:i+batch_size]\n",
    "            one_hot_y[np.arange(batch_size),batch_y] = 1  #convert to one hot encoding\n",
    "            batch_y = one_hot_y\n",
    "            one_hot_y=np.zeros((batch_size,n_classes))  #resetting it for future use\n",
    "            yield batch_x,batch_y\n",
    "        i = ( i + batch_size ) % len(X_train)\n",
    "train_next_batch = next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sethuiyer\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.452737899\n",
      "Train-Accuracy: 0.916016 Train-Loss: 0.278928\n",
      "Validation-Accuracy: 0.910156 Val-Loss: 0.283022 \n",
      "\n",
      "Epoch: 0002 cost= 0.289358502\n",
      "Train-Accuracy: 0.935547 Train-Loss: 0.210925\n",
      "Validation-Accuracy: 0.923828 Val-Loss: 0.278436 \n",
      "\n",
      "Epoch: 0003 cost= 0.257992939\n",
      "Train-Accuracy: 0.9375 Train-Loss: 0.238453\n",
      "Validation-Accuracy: 0.925781 Val-Loss: 0.22384 \n",
      "\n",
      "Epoch: 0004 cost= 0.237515091\n",
      "Train-Accuracy: 0.943359 Train-Loss: 0.181955\n",
      "Validation-Accuracy: 0.933594 Val-Loss: 0.232625 \n",
      "\n",
      "Epoch: 0005 cost= 0.221627768\n",
      "Train-Accuracy: 0.943359 Train-Loss: 0.202629\n",
      "Validation-Accuracy: 0.949219 Val-Loss: 0.23485 \n",
      "\n",
      "Epoch: 0006 cost= 0.207932187\n",
      "Train-Accuracy: 0.953125 Train-Loss: 0.204773\n",
      "Validation-Accuracy: 0.957031 Val-Loss: 0.161737 \n",
      "\n",
      "Epoch: 0007 cost= 0.195618209\n",
      "Train-Accuracy: 0.957031 Train-Loss: 0.144979\n",
      "Validation-Accuracy: 0.962891 Val-Loss: 0.140367 \n",
      "\n",
      "Epoch: 0008 cost= 0.184168392\n",
      "Train-Accuracy: 0.96875 Train-Loss: 0.122484\n",
      "Validation-Accuracy: 0.929688 Val-Loss: 0.226864 \n",
      "\n",
      "Epoch: 0009 cost= 0.173342479\n",
      "Train-Accuracy: 0.941406 Train-Loss: 0.188802\n",
      "Validation-Accuracy: 0.96875 Val-Loss: 0.136564 \n",
      "\n",
      "Epoch: 0010 cost= 0.163184424\n",
      "Train-Accuracy: 0.976563 Train-Loss: 0.101819\n",
      "Validation-Accuracy: 0.966797 Val-Loss: 0.161194 \n",
      "\n",
      "Epoch: 0011 cost= 0.153855678\n",
      "Train-Accuracy: 0.974609 Train-Loss: 0.129327\n",
      "Validation-Accuracy: 0.953125 Val-Loss: 0.162509 \n",
      "\n",
      "Epoch: 0012 cost= 0.145196939\n",
      "Train-Accuracy: 0.958984 Train-Loss: 0.135211\n",
      "Validation-Accuracy: 0.966797 Val-Loss: 0.142844 \n",
      "\n",
      "Epoch: 0013 cost= 0.137118373\n",
      "Train-Accuracy: 0.966797 Train-Loss: 0.126028\n",
      "Validation-Accuracy: 0.980469 Val-Loss: 0.112326 \n",
      "\n",
      "Epoch: 0014 cost= 0.129475393\n",
      "Train-Accuracy: 0.980469 Train-Loss: 0.100034\n",
      "Validation-Accuracy: 0.970703 Val-Loss: 0.145786 \n",
      "\n",
      "Epoch: 0015 cost= 0.122335921\n",
      "Train-Accuracy: 0.972656 Train-Loss: 0.12627\n",
      "Validation-Accuracy: 0.958984 Val-Loss: 0.119761 \n",
      "\n",
      "Epoch: 0016 cost= 0.115718366\n",
      "Train-Accuracy: 0.964844 Train-Loss: 0.11077\n",
      "Validation-Accuracy: 0.958984 Val-Loss: 0.1511 \n",
      "\n",
      "Epoch: 0017 cost= 0.109626588\n",
      "Train-Accuracy: 0.964844 Train-Loss: 0.129016\n",
      "Validation-Accuracy: 0.976563 Val-Loss: 0.0925002 \n",
      "\n",
      "Epoch: 0018 cost= 0.103816105\n",
      "Train-Accuracy: 0.978516 Train-Loss: 0.0755353\n",
      "Validation-Accuracy: 0.976563 Val-Loss: 0.0831641 \n",
      "\n",
      "Epoch: 0019 cost= 0.098371485\n",
      "Train-Accuracy: 0.982422 Train-Loss: 0.0731136\n",
      "Validation-Accuracy: 0.982422 Val-Loss: 0.0633602 \n",
      "\n",
      "Epoch: 0020 cost= 0.093302817\n",
      "Train-Accuracy: 0.984375 Train-Loss: 0.0540941\n",
      "Validation-Accuracy: 0.982422 Val-Loss: 0.0789231 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next(train_next_batch)\n",
    "            batch_x = scaler.transform(batch_x)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y, dropoutRate: 0.0, is_training:True})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\",\"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "            accTrain, costTrain, summary = sess.run([accuracy, cost, merged_summary_op], \n",
    "                                                        feed_dict={x: batch_x, y: batch_y, \n",
    "                                                                   dropoutRate: 0.1, is_training:False})\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            \n",
    "            print(\"Train-Accuracy:\", accTrain,\"Train-Loss:\", costTrain)\n",
    "\n",
    "            batch_x_test, batch_y_test = next(train_next_batch)\n",
    "            batch_x_test = scaler.transform(batch_x_test)\n",
    "\n",
    "            accTest, costVal = sess.run([accuracy, cost], feed_dict={x: batch_x_test, y: batch_y_test, \n",
    "                                                                   dropoutRate: 0.0, is_training:False})\n",
    "\n",
    "            print(\"Validation-Accuracy:\", accTest,\"Val-Loss:\", costVal,\"\\n\")\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess,'./mnist_snn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist_snn\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('./mnist_snn.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD8CAYAAADAKumpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj6ACkB5BBSC9/wfygCxI+6+UPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfa9fad9400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label : 7\n",
      "Predicted Label :  7\n"
     ]
    }
   ],
   "source": [
    "def predict(i):\n",
    "    example_x=train_data.ix[i][1:]\n",
    "    example_y=train_data.ix[i][0]\n",
    "    visualize_digits(i)\n",
    "    example_x=example_x.values.reshape([-1,784])\n",
    "    print('Actual Label :', example_y)\n",
    "    preds = sess.run([correct_pred],feed_dict={x:example_x,dropoutRate: 0.0, is_training:False})\n",
    "    print ('Predicted Label : ', preds[0][0])\n",
    "predict(1022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
